{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install litellm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Oxnl8Fr16vYz",
        "outputId": "9e747c0e-1284-4d85-b3e5-0838a50eb5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: litellm in /usr/local/lib/python3.11/dist-packages (1.59.10)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm) (3.11.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.1.8)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.27.2)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.59.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.10.6)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.0.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.8.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.22.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm) (0.8.2)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "-h8Ga5cR7SvV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "ANTHROPIC_API_KEY = userdata.get(\"ANTHROPIC_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "import os\n",
        "\n",
        "## set ENV variables\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "I3T0Fuh38AUG"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Calls without function"
      ],
      "metadata": {
        "id": "WD0Etzt-VwfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai_response = completion(\n",
        "  model=\"openai/gpt-4o-mini\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n",
        ")\n",
        "print(openai_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "L88KwWC48JYK",
        "outputId": "28c87067-65da-4223-fb26-73d93b147dfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm just a virtual assistant, but I'm here and ready to help you. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Calls without function with streaming"
      ],
      "metadata": {
        "id": "mWnZbCk3PlXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai__response = completion(\n",
        "  model=\"openai/gpt-4o\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
        "  stream=True,\n",
        ")\n",
        "print(openai__response)\n",
        "for chunk in openai__response:\n",
        "    print(chunk[\"choices\"][0][\"delta\"][\"content\"])"
      ],
      "metadata": {
        "id": "rxWiG0qF-ooR",
        "outputId": "6862c579-3bb5-4f90-92d9-8b824e807f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x7f05d8b86090>\n",
            "Hello\n",
            "!\n",
            " I'm\n",
            " just\n",
            " a\n",
            " program\n",
            ",\n",
            " so\n",
            " I\n",
            " don't\n",
            " have\n",
            " feelings\n",
            ",\n",
            " but\n",
            " I'm\n",
            " here\n",
            " and\n",
            " ready\n",
            " to\n",
            " help\n",
            " you\n",
            ".\n",
            " How\n",
            " can\n",
            " I\n",
            " assist\n",
            " you\n",
            " today\n",
            "?\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Openai parallel function calling without streaming"
      ],
      "metadata": {
        "id": "ZRXstYF4PrSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
        "    elif \"paris\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        ")\n",
        "print(\"\\nLLM Response1:\\n\", response)\n",
        "response_message = response.choices[0].message\n",
        "tool_calls = response.choices[0].message.tool_calls\n",
        "\n",
        "\n",
        "\n",
        "## Parse the Model Response and Execute Functions\n",
        "\n",
        "\n",
        "# Check if the model wants to call a function\n",
        "if tool_calls:\n",
        "    # Execute the functions and prepare responses\n",
        "    available_functions = {\n",
        "        \"get_current_weather\": get_current_weather,\n",
        "    }\n",
        "\n",
        "    messages.append(response_message)  # Extend conversation with assistant's reply\n",
        "\n",
        "    for tool_call in tool_calls:\n",
        "      print(f\"\\nExecuting tool call\\n{tool_call}\")\n",
        "      function_name = tool_call.function.name\n",
        "      function_to_call = available_functions[function_name]\n",
        "      function_args = json.loads(tool_call.function.arguments)\n",
        "      # calling the get_current_weather() function\n",
        "      function_response = function_to_call(\n",
        "          location=function_args.get(\"location\"),\n",
        "          unit=function_args.get(\"unit\"),\n",
        "      )\n",
        "      print(f\"Result from tool call\\n{function_response}\\n\")\n",
        "\n",
        "      # Extend conversation with function response\n",
        "      messages.append(\n",
        "          {\n",
        "              \"tool_call_id\": tool_call.id,\n",
        "              \"role\": \"tool\",\n",
        "              \"name\": function_name,\n",
        "              \"content\": function_response,\n",
        "          }\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "final_fun_calling_response = litellm.completion(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=messages,\n",
        ")\n",
        "print(\"final_fun_calling_response\\n\", final_fun_calling_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "2gIKp6TjEGoW",
        "outputId": "7fe7804a-e794-43d4-f0c3-dd88f229176d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLM Response1:\n",
            " ModelResponse(id='chatcmpl-AvjY9AnVv9N6GuLjPuLkplxliFrZF', created=1738323153, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_2f141ce944', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_DpAQgIqnjmtSttv196OR1woj', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_aumhYvcs3xWa51nFpuWcRbtj', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_FhXR2DE8e88F8SMh3mIFsh8E', type='function')], function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=78, prompt_tokens=88, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_DpAQgIqnjmtSttv196OR1woj', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_aumhYvcs3xWa51nFpuWcRbtj', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_FhXR2DE8e88F8SMh3mIFsh8E', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
            "\n",
            "final_fun_calling_response\n",
            " The current weather in San Francisco is 72°F, in Tokyo it is 10°C, and in Paris it is 22°C.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Openai Parallel function calling with streaming"
      ],
      "metadata": {
        "id": "zMdEfLhzQfhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    if \"tokyo\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
        "    elif \"san francisco\" in location.lower():\n",
        "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
        "    elif \"paris\" in location.lower():\n",
        "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
        "    else:\n",
        "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "response = litellm.completion(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
        ")\n",
        "print(\"\\nLLM Response1:\\n\", response)\n",
        "response_message = response.choices[0].message\n",
        "tool_calls = response.choices[0].message.tool_calls\n",
        "\n",
        "\n",
        "\n",
        "## Parse the Model Response and Execute Functions\n",
        "\n",
        "\n",
        "# Check if the model wants to call a function\n",
        "if tool_calls:\n",
        "    # Execute the functions and prepare responses\n",
        "    available_functions = {\n",
        "        \"get_current_weather\": get_current_weather,\n",
        "    }\n",
        "\n",
        "    messages.append(response_message)  # Extend conversation with assistant's reply\n",
        "\n",
        "    for tool_call in tool_calls:\n",
        "      print(f\"\\nExecuting tool call\\n{tool_call}\")\n",
        "      function_name = tool_call.function.name\n",
        "      function_to_call = available_functions[function_name]\n",
        "      function_args = json.loads(tool_call.function.arguments)\n",
        "      # calling the get_current_weather() function\n",
        "      function_response = function_to_call(\n",
        "          location=function_args.get(\"location\"),\n",
        "          unit=function_args.get(\"unit\"),\n",
        "      )\n",
        "      print(f\"Result from tool call\\n{function_response}\\n\")\n",
        "\n",
        "      # Extend conversation with function response\n",
        "      messages.append(\n",
        "          {\n",
        "              \"tool_call_id\": tool_call.id,\n",
        "              \"role\": \"tool\",\n",
        "              \"name\": function_name,\n",
        "              \"content\": function_response,\n",
        "          }\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "final_fun_calling_response = litellm.completion(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"final_fun_calling_response\\n\", final_fun_calling_response)\n",
        "for chunk in final_fun_calling_response:\n",
        "    print(chunk[\"choices\"][0][\"delta\"][\"content\"])"
      ],
      "metadata": {
        "id": "qbXrG6g2KbfA",
        "outputId": "ba51f76e-dae7-49ce-fed9-87a4240c7980",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLM Response1:\n",
            " ModelResponse(id='chatcmpl-Avk8norxKqWr5kQI3OrV2cGMNYpTR', created=1738325425, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_2f141ce944', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_24hlBVlyKAPQ8Xtevn8hbTuM', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_9LvztpyqKP8LgOEYhivVAv0R', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_GeFk8NZUBtJJMkWCeckK2ugO', type='function')], function_call=None, provider_specific_fields={'refusal': None}, refusal=None))], usage=Usage(completion_tokens=84, prompt_tokens=88, total_tokens=172, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_24hlBVlyKAPQ8Xtevn8hbTuM', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_9LvztpyqKP8LgOEYhivVAv0R', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_GeFk8NZUBtJJMkWCeckK2ugO', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
            "\n",
            "final_fun_calling_response\n",
            " <litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x7f05d8ba8990>\n",
            "The\n",
            " current\n",
            " weather\n",
            " in\n",
            " San\n",
            " Francisco\n",
            " is\n",
            " \n",
            "72\n",
            "°F\n",
            ",\n",
            " in\n",
            " Tokyo\n",
            " it\n",
            "'s\n",
            " \n",
            "10\n",
            "°C\n",
            ",\n",
            " and\n",
            " in\n",
            " Paris\n",
            " it\n",
            "'s\n",
            " \n",
            "22\n",
            "°C\n",
            ".\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anthropic without function"
      ],
      "metadata": {
        "id": "JsVeORP6SUAd"
      }
    },
    {
      "source": [
        "## set ENV variables\n",
        "\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
        "\n",
        "anthropic_response = completion(\n",
        "  model=\"anthropic/claude-2\",\n",
        "  messages=[{ \"content\": \"what's the weather in SF\",\"role\": \"user\"}]\n",
        ")\n",
        "print(anthropic_response)\n",
        "print(anthropic_response.choices[0].message.content)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q5MQpD9V9NKu",
        "outputId": "fb24bd88-ab2e-4700-d083-af30fb72b1ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-90552026-f988-4e1c-ab70-5abcfa475faf', created=1738325481, model='claude-2', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop_sequence', index=0, message=Message(content=\" Unfortunately, I do not have up-to-date access to weather conditions in specific locations. As an AI assistant without direct access to meteorological data sources, I cannot provide current weather details for San Francisco or other cities. I'd be happy to discuss other topics I can assist you with though!\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=58, prompt_tokens=13, total_tokens=71, completion_tokens_details=None, prompt_tokens_details=None))\n",
            " Unfortunately, I do not have up-to-date access to weather conditions in specific locations. As an AI assistant without direct access to meteorological data sources, I cannot provide current weather details for San Francisco or other cities. I'd be happy to discuss other topics I can assist you with though!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anthropic without function with streaming"
      ],
      "metadata": {
        "id": "lYM-vrsTSb1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "anthropic_response = completion(\n",
        "  model=\"anthropic/claude-3-sonnet-20240229\",\n",
        "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
        "  stream=True\n",
        ")\n",
        "print(anthropic_response)\n",
        "for chunk in anthropic_response:\n",
        "    print(chunk[\"choices\"][0][\"delta\"][\"content\"])"
      ],
      "metadata": {
        "id": "gQiTCbI3_AF2",
        "outputId": "e50275f6-9309-4626-e210-265b49dc2064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<litellm.litellm_core_utils.streaming_handler.CustomStreamWrapper object at 0x7f05d89bd8d0>\n",
            "Hello\n",
            "! As\n",
            " an AI\n",
            " language\n",
            " model, I don\n",
            "'t have physical sens\n",
            "ations or emotions\n",
            ",\n",
            " but I'm operating\n",
            " properly\n",
            " and ready\n",
            " to assist\n",
            " you with any questions\n",
            " or tasks you may\n",
            " have. How\n",
            " can\n",
            " I help\n",
            " you today?\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anthropic with function"
      ],
      "metadata": {
        "id": "P4JHBvahSh2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]\n",
        "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston today?\"}]\n",
        "\n",
        "# response = completion(\n",
        "#     model=\"anthropic/claude-3-opus-20240229\",\n",
        "#     messages=messages,\n",
        "#     tools=tools,\n",
        "#     tool_choice=\"auto\",\n",
        "# )\n",
        "# # Add any assertions, here to check response args\n",
        "# print(response)\n",
        "# assert isinstance(response.choices[0].message.tool_calls[0].function.name, str)\n",
        "# assert isinstance(\n",
        "#     response.choices[0].message.tool_calls[0].function.arguments, str\n",
        "# )\n",
        "\n",
        "\n",
        "response = completion(\n",
        "    model=\"anthropic/claude-3-opus-20240229\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice={\"type\": \"function\", \"function\": { \"name\": \"get_current_weather\" }}, # Updated tool_choice to force the function call\n",
        ")\n",
        "print(response)\n",
        "\n",
        "# Check if the model wants to call a function\n",
        "if tool_calls:\n",
        "    # Execute the functions and prepare responses\n",
        "    available_functions = {\n",
        "        \"get_current_weather\": get_current_weather,\n",
        "    }\n",
        "\n",
        "    messages.append(response_message)  # Extend conversation with assistant's reply\n",
        "\n",
        "    for tool_call in tool_calls:\n",
        "      print(f\"\\nExecuting tool call\\n{tool_call}\")\n",
        "      function_name = tool_call.function.name\n",
        "      function_to_call = available_functions[function_name]\n",
        "      function_args = json.loads(tool_call.function.arguments)\n",
        "      # calling the get_current_weather() function\n",
        "      function_response = function_to_call(\n",
        "          location=function_args.get(\"location\"),\n",
        "          unit=function_args.get(\"unit\"),\n",
        "      )\n",
        "      print(f\"Result from tool call\\n{function_response}\\n\")\n",
        "\n",
        "      # Extend conversation with function response\n",
        "      messages.append(\n",
        "          {\n",
        "              \"tool_call_id\": tool_call.id,\n",
        "              \"role\": \"tool\",\n",
        "              \"name\": function_name,\n",
        "              \"content\": function_response,\n",
        "          }\n",
        "      )\n",
        "\n",
        "final_fun_calling_response = litellm.completion(\n",
        "    model=\"anthropic/claude-3-opus-20240229\", # Added anthropic/ prefix to the model name\n",
        "    messages=messages,\n",
        "    tools=tools, # Added the tools parameter back into the final completion call\n",
        "    stream=True\n",
        ")\n",
        "# print(\"final_fun_calling_response\\n\", final_fun_calling_response)\n",
        "for chunk in final_fun_calling_response:\n",
        "    print(chunk[\"choices\"][0][\"delta\"][\"content\"])\n"
      ],
      "metadata": {
        "id": "hUvSFMOeOgKq",
        "outputId": "7f2a2a62-df52-44d0-84c9-22cdf3cd91a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-dda7beea-901d-434e-9be2-1face69f0a54', created=1738326446, model='claude-3-opus-20240229', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), id='toolu_01WiZ5MNHhkuDqjy2M59swKx', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=53, prompt_tokens=415, total_tokens=468, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_24hlBVlyKAPQ8Xtevn8hbTuM', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_9LvztpyqKP8LgOEYhivVAv0R', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_GeFk8NZUBtJJMkWCeckK2ugO', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "\n",
            "<thinking>\n",
            "To answer this query, I\n",
            " need to:\n",
            "1. Call\n",
            " the get_current_weather tool with\n",
            " the location set to \"Boston, MA\".\n",
            " The user directly provided the city\n",
            " of Boston in their query.\n",
            "2. They\n",
            " did not specify a unit,\n",
            " but I can infer they\n",
            " likely want the temperature in Fahren\n",
            "heit since that is the standar\n",
            "d unit used in the US. \n",
            "\n",
            "\n",
            "I have all the required parameters to\n",
            " make the necessary API call\n",
            ", so I can proceed with\n",
            " getting the weather for Boston.\n",
            "\n",
            "</thinking>\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "    model=\"anthropic/claude-3-opus-20240229\",\n",
        "    messages=messages,\n",
        "    tools=tools,\n",
        "    tool_choice={\"type\": \"tool\", \"name\": \"get_weather\"},\n",
        ")"
      ],
      "metadata": {
        "id": "Dz9lHVOdS7E4",
        "outputId": "a812605c-5d8d-43a9-c211-62de3b72873a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Invalid tool choice, tool_choice={'type': 'tool', 'name': 'get_weather'}. Please ensure tool_choice follows the OpenAI spec",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-6356b2de2ec0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = completion(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"anthropic/claude-3-opus-20240229\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"tool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"get_weather\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                 )  # DO NOT MAKE THREADED - router retry fallback relies on this!\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m                     \u001b[0mprint_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error while checking max token limit: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0;31m# MODEL CALL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"stream\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/main.py\u001b[0m in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_chat_completion_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;31m# validate tool_choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mtool_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_chat_completion_tool_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m     \u001b[0;31m######### unpacking kwargs #####################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/litellm/utils.py\u001b[0m in \u001b[0;36mvalidate_chat_completion_tool_choice\u001b[0;34m(tool_choice)\u001b[0m\n\u001b[1;32m   5876\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtool_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtool_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5878\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m   5879\u001b[0m                 \u001b[0;34mf\"Invalid tool choice, tool_choice={tool_choice}. Please ensure tool_choice follows the OpenAI spec\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5880\u001b[0m             )\n",
            "\u001b[0;31mException\u001b[0m: Invalid tool choice, tool_choice={'type': 'tool', 'name': 'get_weather'}. Please ensure tool_choice follows the OpenAI spec"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "k2wygzLDTNvr",
        "outputId": "5a3f3a38-c282-4d8a-dcf4-9e8632d75c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelResponse(id='chatcmpl-53163eb4-6bcb-43a5-b2b7-986b7188768a', created=1738326355, model='claude-3-opus-20240229', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Boston, MA\", \"unit\": \"fahrenheit\"}', name='get_current_weather'), id='toolu_01ACuoFccMAGhXZniTaGiBse', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=53, prompt_tokens=1772, total_tokens=1825, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_24hlBVlyKAPQ8Xtevn8hbTuM', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_9LvztpyqKP8LgOEYhivVAv0R', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "Executing tool call\n",
            "ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), id='call_GeFk8NZUBtJJMkWCeckK2ugO', type='function')\n",
            "Result from tool call\n",
            "{\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "652RqUN1THdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}